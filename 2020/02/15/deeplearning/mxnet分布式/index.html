
 <!DOCTYPE HTML>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  
    <title>MXNet分布式 | The Notes of HH</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=3, minimum-scale=1">
    
    <meta name="author" content="John Doe">
    
    <meta name="description" content="1 介绍MXNet是DMLC社区开源的深度学习框架，具有如下特点：    1. 接口清晰，易用    2. 显存利用与多卡并行效率高    3. 部署成熟，PC、移动    4. 支持多语言python、R、matlab、JavaScript等    5. 具有静态图编程接口MXNet和动态图编程接">
    
    
    
    
    <link rel="alternative" href="/Tinnypp/atom.xml" title="The Notes of HH" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/myLogo.png">
    <link rel="apple-touch-icon-precomposed" href="/img/myLogo.png">
    

  
    <link href="/css/font-awesome.min.css" rel="stylesheet">
    
  

    
<link rel="stylesheet" href="/css/style.css">

    <script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?d182ed77fc48758bf45a33835ee35745";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

      <script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
  (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
  e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v1/st.js','_st');

  _st('install','.............Add your swiftype userID...............');
</script>
<meta name="generator" content="Hexo 4.2.0"></head>

  <body>
    <header>
      <div>
		
			<div id="imglogo">
				<a href="/"><img src="/img/logo.svg" alt="The Notes of HH" title="The Notes of HH"/></a>
			</div>
			
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="The Notes of HH">The Notes of HH</a></h1>
				<h2 class="blog-motto">Hold Your Horse</h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="菜单">
			</a></div>
			<nav class="animated">
				<ul>
                    <ul>
					 
						<li><a href="https://hhwode.github.io/">首页</a></li>
					
						<li><a href="https://hhwode.github.io/archives">归档</a></li>
					
					<li>
					
					</li>
                <!--<li><div class="closeaside"><a class="closebutton" href="#" title="隐藏侧边栏"></a></div></li>-->

				</ul>
			</nav>	
</div>
    </header>
    <div id="container" class="clearfix">
      <div id="main" class="post" itemscope itemprop="blogPost">
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2020/02/15/deeplearning/mxnet分布式/" title="MXNet分布式" itemprop="url">MXNet分布式</a>
  </h1>
  <p class="article-time">
    <time datetime="2020-02-15T12:44:24.000Z" itemprop="datePublished">2020-02-15</time>
  </p>
</header>
	<div class="article-content">
		
		
		<div id="toc" class="toc-article">
			<strong class="toc-title"></strong>
		<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-介绍"><span class="toc-number">1.</span> <span class="toc-text">1 介绍</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-分布式"><span class="toc-number">2.</span> <span class="toc-text">2 分布式</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-ps-lite"><span class="toc-number">2.1.</span> <span class="toc-text">2.1 ps-lite</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-相关概念"><span class="toc-number">2.2.</span> <span class="toc-text">2.2 相关概念</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-1-进程类型"><span class="toc-number">2.2.1.</span> <span class="toc-text">2.2.1 进程类型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-2-Kvstore"><span class="toc-number">2.2.2.</span> <span class="toc-text">2.2.2 Kvstore</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-3-Keys的分配"><span class="toc-number">2.2.3.</span> <span class="toc-text">2.2.3 Keys的分配</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-4-切分训练数据集"><span class="toc-number">2.2.4.</span> <span class="toc-text">2.2.4 切分训练数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-5-分布式训练的不同模式"><span class="toc-number">2.2.5.</span> <span class="toc-text">2.2.5 分布式训练的不同模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-6-梯度压缩"><span class="toc-number">2.2.6.</span> <span class="toc-text">2.2.6 梯度压缩</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-如何使用"><span class="toc-number">2.3.</span> <span class="toc-text">2.3 如何使用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-1-launch-py工具"><span class="toc-number">2.3.1.</span> <span class="toc-text">2.3.1 launch.py工具</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-2-手动启动"><span class="toc-number">2.3.2.</span> <span class="toc-text">2.3.2 手动启动</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-4-实用工具"><span class="toc-number">2.4.</span> <span class="toc-text">2.4 实用工具</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-1-其他环境变量使用"><span class="toc-number">2.4.1.</span> <span class="toc-text">2.4.1 其他环境变量使用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-2-Profile"><span class="toc-number">2.4.2.</span> <span class="toc-text">2.4.2 Profile</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-优化"><span class="toc-number">3.</span> <span class="toc-text">3 优化</span></a></li></ol>
		</div>
		
		<h1 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1 介绍"></a>1 介绍</h1><p>MXNet是DMLC社区开源的深度学习框架，具有如下特点：<br>    1. 接口清晰，易用<br>    2. 显存利用与多卡并行效率高<br>    3. 部署成熟，PC、移动<br>    4. 支持多语言python、R、matlab、JavaScript等<br>    5. 具有静态图编程接口MXNet和动态图编程接口gluon（MXNet和gluon可以看出类似TensorFlow和Keras的关系，但gluon是动态图，与其对应的是pytorch）</p>
<h1 id="2-分布式"><a href="#2-分布式" class="headerlink" title="2 分布式"></a>2 分布式</h1><p>数据很大，分布式已经是趋势，节点之间共享参数是必须，面临三个挑战：<br>    1. 如何利用高带宽来获取共享参数<br>    2. 很多机器学习是顺序的，同步和高延迟的阻碍影响了性能<br>    3. 高容错率<br>MXNet<a href="https://mxnet.incubator.apache.org/api/faq/multi_device" target="_blank" rel="noopener">[1]</a>[2]基于参数服务器结构上进行了改进，提出的ps-lite分布式训练结构。</p>
<h2 id="2-1-ps-lite"><a href="#2-1-ps-lite" class="headerlink" title="2.1 ps-lite"></a>2.1 ps-lite</h2><p>ps-lite<a href="https://www.cnblogs.com/heguanyou/p/7868596.html" target="_blank" rel="noopener">[3]</a> <a href="https://ps-lite.readthedocs.io/en/latest/how_to.html" target="_blank" rel="noopener">[4]</a>包含三种角色：Worker、Server、Scheduler（默认基于服务器级别作为Worker）。具体关系如下图：<br> <img src="/images/mxnet-distribute/1.png" alt="" title="ps-lite"><br>    - Worker节点负责计算参数，并发参数push到Server，同时从Server pull参数回来。<br>    - Server节点负责管理Worker节点发送来的参数，并“合并”，之后供各个Worker使用。<br>    - Scheduler节点负责管理Worker节点和Server节点的状态，Worker与Server之间的连接是通过Scheduler的。<br>ps-lite支持两种分布式计算模式，数据并行和模型并行，并同时支持同步和异步参数更新。<br>    - 数据并行，指的是每个设备存储完整模型副本的情况，每个设备使用数据集的不同部分进行工作，设备共同更新共享模型。<br>    - 模型并行，不同的设备被分配了学习模型不同部分的任务。 目前，MXNet仅支持单机中的模型并行。</p>
<h2 id="2-2-相关概念"><a href="#2-2-相关概念" class="headerlink" title="2.2 相关概念"></a>2.2 相关概念</h2><h3 id="2-2-1-进程类型"><a href="#2-2-1-进程类型" class="headerlink" title="2.2.1 进程类型"></a>2.2.1 进程类型</h3><p>MXNet中有三种进程类型，这些进程之间相互通信，完成模型的训练。<br>    - Worker：Worker节点实际上在一批训练样本上进行训练。 在处理每个批次之前，Workers从服务器上拉出权重。 Worker还会在每个batch处理后向服务器发送梯度(gradient)。 根据训练模型的工作量，在同一台机器上运行多个工作进程可能不是一个好主意，MXNet默认以机器作为Worker的粒度，减少了参数push/pull操作。<br>    - Server：可以有多个Servers存储模型的参数，并与Workers进行通信。 Server可能与Worker进程同处一处,也可能不。如果只有一个Server，模型的所有参数都存储在该Server上，如果有多个，会依据key值进行随机划分模型参数，每个Server存储一部分。<br>    - Scheduler(调度器)：只有一个Scheduler。Scheduler的作用是配置集群。这包括等待每个节点启动以及节点正在监听哪个端口之类的消息。 然后Scheduler让所有进程知道集群中的其他节点的信息，以便它们可以相互通信。优先启动的进程。</p>
<h3 id="2-2-2-Kvstore"><a href="#2-2-2-Kvstore" class="headerlink" title="2.2.2 Kvstore"></a>2.2.2 Kvstore</h3><p>分布式训练关键部分，Servers将参数存储为K-V形式，以便Workers向Servers进行push/pull操作。<br>需要指定给训练器，<br>    1. MXNet情况如下，只有Module才有分布式参数kvstore：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mod &#x3D; mx.mod.Module(mlp)  # mlp是最后分类层</span><br><span class="line">mod.bind(data_shapes&#x3D;train_iter.provide_data,</span><br><span class="line">		 label_shapes&#x3D;train_iter.provide_label)</span><br><span class="line">mod.init_params()</span><br><span class="line">kv &#x3D; mx.kvstore.create(&#39;dist_sync&#39;)</span><br><span class="line">mod.fit(train_iter, eval_data&#x3D;val_iter,optimizer_params&#x3D;&#123;&#39;learning_rate&#39;:0.01, &#39;momentum&#39;: 0.9&#125;,num_epoch&#x3D;2, kvstore&#x3D;kv)</span><br></pre></td></tr></table></figure>
<pre><code>或者：</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mod.fit(train_iter, eval_data&#x3D;val_iter,optimizer_params&#x3D;&#123;&#39;learning_rate&#39;:0.01, &#39;momentum&#39;: 0.9&#125;,num_epoch&#x3D;2, kvstore&#x3D;&quot;dist_sync&quot;)</span><br></pre></td></tr></table></figure>
<pre><code>2. gluon情况下：</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trainer = gluon.Trainer(net.collect_params(),<span class="string">'sgd'</span>,&#123;<span class="string">'learning_rate'</span> : lr&#125;,kvstore=<span class="string">'dist_sync'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>或者如下，可以用kv.rank知道哪个worker</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kv = mx.kvstore.create(<span class="string">'dist_sync'</span>)</span><br><span class="line">trainer = gluon.Trainer(net.collect_params(),<span class="string">'sgd'</span>,&#123;<span class="string">'learning_rate'</span> : lr&#125;, kvstore=kv)</span><br></pre></td></tr></table></figure>

<h3 id="2-2-3-Keys的分配"><a href="#2-2-3-Keys的分配" class="headerlink" title="2.2.3 Keys的分配"></a>2.2.3 Keys的分配</h3><p>每个Server不一定存储所有的key或全部的参数数组。 参数分布在不同的Server上。 哪个Server存储特定的keys是随机决定的。 KVStore透明地处理不同服务器上的keys分配。 它确保当一个keys被拉取时，该请求被发送到的服务器具有对应value。 如果某个keys的值非常大，则可能会在不同的服务器上分片。这意味着不同的服务器拥有不同部分的value。<br>我们之前跑的是单个Server情况，可以尝试多个Server，看其参数划分后，通信开销是否减少，性能是否有提高。</p>
<h3 id="2-2-4-切分训练数据集"><a href="#2-2-4-切分训练数据集" class="headerlink" title="2.2.4 切分训练数据集"></a>2.2.4 切分训练数据集</h3><p>单个Worker的数据并行训练，我们可以使用mxnet.gluon.utils.split_and_load来切分数据迭代器(data iterator)提供的一批样本，然后将该批处理的每个部分加载到将处理它的设备上。<br>分布式训练的情况下，我们需要在开始时将数据集分成n个部分，以便每个Worker获得不同的部分。然后，每个Worker可以使用split_and_load再次将数据集的这部分划分到单个机器上的不同设备上。</p>
<h3 id="2-2-5-分布式训练的不同模式"><a href="#2-2-5-分布式训练的不同模式" class="headerlink" title="2.2.5 分布式训练的不同模式"></a>2.2.5 分布式训练的不同模式</h3><p>使用不同类型的kvstore可以启用不同的分布式训练模式：<br>    - dist_sync：同步分布式训练，当每个节点上使用多个GPU时使用，此模式在CPU上聚合梯度并更新权重。<br>    - dist_async：异步分布式训练。<br>    - dist_sync_device：与dist_sync相同，当每个节点上使用多个GPU时使用，此模式在GPU上聚合梯度并更新权重。<br>    - dist_async_device：与dist_sync_device相似，但是是异步模式。</p>
<h3 id="2-2-6-梯度压缩"><a href="#2-2-6-梯度压缩" class="headerlink" title="2.2.6 梯度压缩"></a>2.2.6 梯度压缩</h3><p>当通信费用昂贵，并且计算时间与通信时间的比例较低时，通信可能成为瓶颈。 在这种情况下，可以使用梯度压缩来降低通信成本，从而加速训练<a href="https://mxnet.incubator.apache.org/api/faq/gradient_compression" target="_blank" rel="noopener">[5]</a>。<br>怎么压缩的<br>如何使用</p>
<h2 id="2-3-如何使用"><a href="#2-3-如何使用" class="headerlink" title="2.3 如何使用"></a>2.3 如何使用</h2><p>MXNet提供了一个执行分布式训练的脚本，该工具是在mxnet安装位置下的tools/launch.py，可以方便执行，并介绍了手动启动分布式训练的方法，设置对应环境变量即可<a href="https://mxnet.incubator.apache.org/api/faq/multi_device" target="_blank" rel="noopener">[1]</a>。</p>
<h3 id="2-3-1-launch-py工具"><a href="#2-3-1-launch-py工具" class="headerlink" title="2.3.1 launch.py工具"></a>2.3.1 launch.py工具</h3><p>MXNet提供了一个脚本工具/ launch.py，以便于开展分布式训练工作。这支持各种类型的集群资源管理器，如ssh，mpirun，yarn和sge。launch.py工具只是针对服务器级别为Worker，因为其只起了一个进程，如果要起多个进程，如何指定一个进程对应一个GPU，比较难设置。<br>比如在单机上运行脚本：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#python image_classification.py --dataset cifar10 --model vgg11 --num-epochs 1</span><br></pre></td></tr></table></figure>
<p>此示例的分布式训练，可执行以下操作： 如果包含脚本image_classification.py的mxnet目录可供集群中的所有计算机访问（例如，如果它们位于网络文件系统上），则可以运行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#..&#x2F;..&#x2F;tools&#x2F;launch.py -n 3 -H hosts --launcher ssh python3 mnist.py --kvstore dist_sync</span><br></pre></td></tr></table></figure>
<p>如果包含脚本的目录不能从集群中的其他机器访问，那么我们可以将当前目录同步到所有机器。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#..&#x2F;..&#x2F;tools&#x2F;launch.py -n 3 -H hosts --launcher ssh --sync-dst-dir &#x2F;tmp&#x2F;mxnet_job&#x2F; python3 mnist.py --kvstore dist_sync</span><br></pre></td></tr></table></figure>
<p>launch.py提交分布式训练工具参数：<br>    <strong>-n</strong> 表示要启动的worker节点的数量。<br>    <strong>-s</strong> 表示要启动的server节点的数量。未指定时，默认等于Worker节点的数量<br>    <strong>–launcher</strong> 表示通信模式。选项有：<br>        ssh 如果机器可以通过ssh进行通信而无需密码。 这是默认启动模式。<br>        mpi 使用Open MPI时开启<br>        sge 适用于Sun Grid引擎<br>        yarn 适用于Apache yarn<br>        local 用于在同一本地计算机上启动所有进程。 这可以用于调试。<br>    <strong>-H</strong> 需要主机文件的路径,该文件包含集群中机器的IP，一行一个IP。这些机器应能够在不使用密码的情况下相互通信。 此文件仅适用于启动程序模式为ssh或mpi时<br>    <strong>–sync-dst-dir</strong> 将所有主机上的一个目录的路径指向当前将被同步的工作目录。此选项仅支持ssh启动模式。</p>
<h3 id="2-3-2-手动启动"><a href="#2-3-2-手动启动" class="headerlink" title="2.3.2 手动启动"></a>2.3.2 手动启动</h3><p>MXNet使用环境变量将不同的角色分配给不同的进程，并让不同的进程查找调度程序，launch.py工具就是将这个手动分配的工作进行封装，不需要用户自己手动指定。<br>包含如下几个环境变量需要设置：<br>    <strong>DMLC_ROLE</strong> ：指定进程的角色。 这可以是server、worker或scheduler。注意，只有一个scheduler。 当DMLC_ROLE设置为server或scheduler后，这些进程在导入mxnet时启动。<br>    <strong>DMLC_PS_ROOT_URI</strong> ：指定scheduler的IP<br>    <strong>DMLC_PS_ROOT_PORT</strong> ：指定scheduler侦听的端口<br>    <strong>DMLC_NUM_SERVER</strong> ：指定群集中有多少个server节点<br>    <strong>DMLC_NUM_WORKER</strong> ：指定群集中有多少个worker节点</p>
<p>启动方式，比如启动1个scheduler，2个server，2个worker：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#DMLC_ROLE&#x3D;scheduler DMLC_PS_ROOT_URI&#x3D;127.0.0.1 DMLC_PS_ROOT_PORT&#x3D;9092 DMLC_NUM_SERVER&#x3D;2 DMLC_NUM_WORKER&#x3D;2 python3 mnist.py --kv-store dist_sync</span><br><span class="line"></span><br><span class="line">#DMLC_ROLE&#x3D;server DMLC_PS_ROOT_URI&#x3D;127.0.0.1 DMLC_PS_ROOT_PORT&#x3D;9092 DMLC_NUM_SERVER&#x3D;2 DMLC_NUM_WORKER&#x3D;2 python3 mnist.py --kv-store dist_sync</span><br><span class="line"></span><br><span class="line">#DMLC_ROLE&#x3D;server DMLC_PS_ROOT_URI&#x3D;127.0.0.1 DMLC_PS_ROOT_PORT&#x3D;9092 DMLC_NUM_SERVER&#x3D;2 DMLC_NUM_WORKER&#x3D;2 python3 mnist.py --kv-store dist_sync</span><br><span class="line"></span><br><span class="line">#DMLC_ROLE&#x3D;worker DMLC_PS_ROOT_URI&#x3D;127.0.0.1 DMLC_PS_ROOT_PORT&#x3D;9092 DMLC_NUM_SERVER&#x3D;2 DMLC_NUM_WORKER&#x3D;2 python3 mnist.py --kv-store dist_sync</span><br><span class="line"></span><br><span class="line">#DMLC_ROLE&#x3D;worker DMLC_PS_ROOT_URI&#x3D;127.0.0.1 DMLC_PS_ROOT_PORT&#x3D;9092 DMLC_NUM_SERVER&#x3D;2 DMLC_NUM_WORKER&#x3D;2 python3 mnist.py --kv-store dist_sync</span><br></pre></td></tr></table></figure>
<h2 id="2-4-实用工具"><a href="#2-4-实用工具" class="headerlink" title="2.4 实用工具"></a>2.4 实用工具</h2><h3 id="2-4-1-其他环境变量使用"><a href="#2-4-1-其他环境变量使用" class="headerlink" title="2.4.1 其他环境变量使用"></a>2.4.1 其他环境变量使用</h3><ol>
<li><strong>PS_VERBOSE</strong>：输出通信消息<br> PS_VERBOSE=1: logging connection information<br> PS_VERBOSE=2: logging all data communication information</li>
<li><strong>DMLC_INTERFACE</strong>：指定通信网卡<br> DMLC_INTERFACE=br0<br>更多环境变量设置见<a href="https://mxnet.incubator.apache.org/api/faq/env_var" target="_blank" rel="noopener">[6]</a>。<h3 id="2-4-2-Profile"><a href="#2-4-2-Profile" class="headerlink" title="2.4.2 Profile"></a>2.4.2 Profile</h3>类似timeline的工具，可生成profile.json<a href="https://mxnet.apache.org/api/python/docs/tutorials/performance/backend/profiler.html" target="_blank" rel="noopener">[7]</a>文件，在Chrome浏览器上输入chrome://tracing/进行加载该json文件。</li>
<li><strong>MXNET_PROFILER_AUTOSTART=1</strong>：指定环境变量运行mxnet程序，可以不需要修改代码，将把整个程序执行情况统计，文件会变成很大。如下图跑一个epoch的mnist分类情况。<br><img src="/images/mxnet-distribute/2.png" alt="" title="profile"></li>
<li><strong>代码指定</strong><br>代码指定的方式，可自定义统计部分，文件容量相对较少。<br>创建profile配置<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from mxnet import profiler</span><br><span class="line">profiler.set_config(profile_all&#x3D;True,</span><br><span class="line">			   aggregate_stats&#x3D;True,</span><br><span class="line">			   continuous_dump&#x3D;True,</span><br><span class="line">			   filename&#x3D;&#39;profile_output.json&#39;)</span><br></pre></td></tr></table></figure>
指定统计范围<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">profiler.set_state(&#39;run&#39;) # 开始统计</span><br><span class="line">run_training_iteration(*next(itr)) # 训练</span><br><span class="line">mx.nd.waitall() </span><br><span class="line">profiler.set_state(&#39;stop&#39;) # 结束统计</span><br><span class="line">profiler.dump()</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h1 id="3-优化"><a href="#3-优化" class="headerlink" title="3 优化"></a>3 优化</h1><p>MKL</p>
<p><strong>Reference</strong><br>    [1] <a href="https://mxnet.incubator.apache.org/api/faq/multi_device" target="_blank" rel="noopener">https://mxnet.incubator.apache.org/api/faq/multi_device</a><br>    [2] Scaling Distributed Machine Learning with the Parameter Server<br>    [3] <a href="https://www.cnblogs.com/heguanyou/p/7868596.html" target="_blank" rel="noopener">https://www.cnblogs.com/heguanyou/p/7868596.html</a><br>    [4] <a href="https://ps-lite.readthedocs.io/en/latest/how_to.html" target="_blank" rel="noopener">https://ps-lite.readthedocs.io/en/latest/how_to.html</a><br>    [5] <a href="https://mxnet.incubator.apache.org/api/faq/gradient_compression" target="_blank" rel="noopener">https://mxnet.incubator.apache.org/api/faq/gradient_compression</a><br>    [6] <a href="https://mxnet.incubator.apache.org/api/faq/env_var" target="_blank" rel="noopener">https://mxnet.incubator.apache.org/api/faq/env_var</a><br>    [7] <a href="https://mxnet.apache.org/api/python/docs/tutorials/performance/backend/profiler.html" target="_blank" rel="noopener">https://mxnet.apache.org/api/python/docs/tutorials/performance/backend/profiler.html</a></p>
<p><strong>附录</strong>：<br>一、MXNet建立模型<br>MXnet中定义好symbol、写好dataiter并且准备好data之后，就可以开开心的去训练了。一般训练一个网络有两种常用的策略，基于model的和基于module的。</p>
<ol>
<li><strong>使用Model构建模型</strong><br>从官方文档里面拿出来的代码看一下（Model形式不能用于分布式训练，没找到kvstore参数接口）：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># 1、configure a two layer neural network</span><br><span class="line">	data &#x3D; mx.symbol.Variable(&#39;data&#39;)</span><br><span class="line">	fc1 &#x3D; mx.symbol.FullyConnected(data, name&#x3D;&#39;fc1&#39;, num_hidden&#x3D;128)</span><br><span class="line">	act1 &#x3D; mx.symbol.Activation(fc1, name&#x3D;&#39;relu1&#39;, act_type&#x3D;&#39;relu&#39;)</span><br><span class="line">	fc2 &#x3D; mx.symbol.FullyConnected(act1, name&#x3D;&#39;fc2&#39;, num_hidden&#x3D;64)</span><br><span class="line">	softmax &#x3D; mx.symbol.SoftmaxOutput(fc2, name&#x3D;&#39;sm&#39;)</span><br><span class="line"># 2、create a model using sklearn-style two-step way</span><br><span class="line">#创建一个model</span><br><span class="line">   model &#x3D; mx.model.FeedForward(</span><br><span class="line">		 softmax,</span><br><span class="line">		 num_epoch&#x3D;num_epoch,</span><br><span class="line">		 learning_rate&#x3D;0.01)</span><br><span class="line">#开始训练</span><br><span class="line">	model.fit(X&#x3D;data_set)</span><br></pre></td></tr></table></figure>
具体的API参照<a href="http://mxnet.io/api/python/model.html" target="_blank" rel="noopener">官网Model介绍</a>。Model形式可定制化不强，不常用，一般用Module构建模型。</li>
<li><strong>使用Module构建模型</strong><br>Module有四种状态：<ol>
<li>初始化状态，就是显存还没有被分配，基本上啥都没做的状态。</li>
<li>bind，在把data和label的shape传到Bind函数里并且执行之后，显存就分配好了，可以准备好计算能力。</li>
<li>参数初始化。就是初始化参数</li>
<li>Optimizer installed 。就是传入SGD，Adam这种optimuzer中去进行训练　<br>构建代码（Module也是可以用于分布式训练的）：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import mxnet as mx</span><br><span class="line"># construct a simple MLP</span><br><span class="line">data &#x3D; mx.symbol.Variable(&#39;data&#39;)</span><br><span class="line">fc1  &#x3D; mx.symbol.FullyConnected(data, name&#x3D;&#39;fc1&#39;, num_hidden&#x3D;128)</span><br><span class="line">act1 &#x3D; mx.symbol.Activation(fc1, name&#x3D;&#39;relu1&#39;, act_type&#x3D;&quot;relu&quot;)</span><br><span class="line">fc2  &#x3D; mx.symbol.FullyConnected(act1, name &#x3D; &#39;fc2&#39;, num_hidden &#x3D; 64)</span><br><span class="line">act2 &#x3D; mx.symbol.Activation(fc2, name&#x3D;&#39;relu2&#39;, act_type&#x3D;&quot;relu&quot;)</span><br><span class="line">fc3  &#x3D; mx.symbol.FullyConnected(act2, name&#x3D;&#39;fc3&#39;, num_hidden&#x3D;10)</span><br><span class="line">out  &#x3D; mx.symbol.SoftmaxOutput(fc3, name &#x3D; &#39;softmax&#39;)</span><br><span class="line"> </span><br><span class="line"># construct the module</span><br><span class="line">mod &#x3D; mx.mod.Module(out)</span><br><span class="line"></span><br><span class="line"># mod.bind的操作是在显卡上分配所需的显存，所以需要把data_shapehe label_shape传递给他</span><br><span class="line">mod.bind(data_shapes&#x3D;train_dataiter.provide_data,</span><br><span class="line">        label_shapes&#x3D;train_dataiter.provide_label)</span><br><span class="line"></span><br><span class="line">mod.init_params()</span><br><span class="line">mod.fit(train_dataiter, eval_data&#x3D;eval_dataiter,</span><br><span class="line">      optimizer_params&#x3D;&#123;&#39;learning_rate&#39;:0.01, &#39;momentum&#39;: 0.9&#125;,</span><br><span class="line">      num_epoch&#x3D;n_epoch)</span><br></pre></td></tr></table></figure>
参考：<a href="https://www.cnblogs.com/daihengchen/p/6506386.html" target="_blank" rel="noopener">https://www.cnblogs.com/daihengchen/p/6506386.html</a></li>
</ol>
</li>
</ol>
  
	</div>
		<footer class="article-footer clearfix">

  <div class="article-tags">
  
  <span></span> <a href="/tags/introduce/">introduce</a>
  </div>


<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/deeplearning/">deeplearning</a>►<a class="article-category-link" href="/categories/deeplearning/distributedTraining/">distributedTraining</a>
</div>



<div class="article-share" id="share">

  <div data-url="https://hhwode.github.io/2020/02/15/deeplearning/mxnet%E5%88%86%E5%B8%83%E5%BC%8F/" data-title="MXNet分布式 | The Notes of HH" data-tsina="" class="share clearfix">
  </div>

</div>
</footer>   	       
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/2020/02/16/deeplearning/MXNET-MPI Embedding MPI parallelism in Parameter Server Task Model for scaling Deep Learning/" title="MXNET-MPI Embedding MPI parallelism in Parameter Server Task Model for scaling Deep Learning">
  <strong>新一篇:</strong><br/>
  <span>
  MXNET-MPI Embedding MPI parallelism in Parameter Server Task Model for scaling Deep Learning</span>
</a>
</div>


<div class="next">
<a href="/2020/02/14/deeplearning/A Comparison of Distributed Machine Learning Platforms/"  title="A Comparison of Distributed Machine Learning Platforms">
 <strong>旧一篇:</strong><br/> 
 <span>A Comparison of Distributed Machine Learning Platforms
</span>
</a>
</div>

</nav>

	
<section class="comment">
	
	<div class="ds-thread" data-title="MXNet分布式" data-thread-key="deeplearning/mxnet分布式" data-author-key="John Doe" data-url="https://hhwode.github.io/post/deeplearning/mxnet分布式"></div>
	
</section>


</div>  
    </div>
    <footer><div id="footer" >
	<div class="copyright">
		<span>Powered by <a href="https://github.com/hexojs/hexo" target="_blank" rel="noopener">Hexo</a> and theme by 
		<a href="https://github.com/levonlin/Tinnypp" target="_blank" rel="noopener">Tinnypp</a>.</span>
		
			<span>© HH</span>
		
	<div>
</div></footer>
    <script src="/js/jquery-2.1.0.min.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  //back to top
  function backToTop(){
    var buttonHTML = $("<a href=\"#top\" id=\"back-top\">" + "<span>Back to Top</span></a>");
    buttonHTML.appendTo($("body"));
    var buttonToTop = $("#back-top");
    // hide #back-top first
    buttonToTop.hide();

    // fade in #back-top
    $(function() {
        $(window).scroll(function() {
            if ($(this).scrollTop() > 200) {
                buttonToTop.fadeIn();
            } else {
                buttonToTop.fadeOut();
            }
        });
        // scroll body to 0px on click
        buttonToTop.click(function() {
            $('body,html').animate({
                scrollTop: 0
            }, 800);
            return false;
        });
    });
  }
  backToTop();

  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      ta = $('#toc.toc-aside');
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else
    {
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
        
    }
  });

  var show = true;
  c.click(function(){
    if(show == true){
        a.addClass('fadeOut').css('display', 'none');
        ta.css('display', 'block').addClass('fadeIn');
        m.addClass('moveMain');  
    }else{
        a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');     
        ta.css('display', 'none'); 
        m.removeClass('moveMain');
        $('#toc.toc-aside').css('display', 'none');
    }
    show = !show;
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      h  = $('article h2')
      ah = $('article h2'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  if(ah.length==0){
    t.css('display','none');
  }else{

    $(window).scroll(function(){
      ta.css("top",Math.max(140,240-$(this).scrollTop()));
    });
  };
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina');
  var html = [
  '<a href="#" class="overlay" id="qrcode"></a>',
  '<div class="qrcode clearfix"><span>扫描二维码分享到微信朋友圈</span><a class="qrclose" href="#share"></a><strong>Loading...Please wait</strong><img id="qrcode-pic" data-src="http://s.jiathis.com/qrcode.php?url=' + encodedUrl + '"/></div>',
  '<a href="#textlogo" class="article-back-to-top" title="Top"></a>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="QRcode"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="Weibo"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);
  $('.article-share-qrcode').click(function(){
    var imgSrc = $('#qrcode-pic').attr('data-src');
    $('#qrcode-pic').attr('src', imgSrc);
    $('#qrcode-pic').load(function(){
        $('.qrcode strong').text(' ');
    });
  });
});     
</script>


<script type="text/javascript">
  var duoshuoQuery = {short_name:"tinnypp"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = '//static.duoshuo.com/embed.js';    //change to ds.src = '/js/embed.js'; to add useragent for duoshuo
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
    || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
</script> 



<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>

<script type="text/javascript">
  function footerPosition() {
    var contentHeight = document.documentElement.scrollHeight,
        winHeight = window.innerHeight;
    if(contentHeight <= winHeight) {
      $('footer').addClass('fixed-bottom');
    } else {
      $('footer').removeClass('fixed-bottom');
    }
  }
  footerPosition();
  $(window).resize(footerPosition);
</script>


  </body>
</html>
