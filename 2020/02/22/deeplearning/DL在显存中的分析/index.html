
 <!DOCTYPE HTML>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  
    <title>Deep Learning的模型在GPU显存中的分析 | The Notes of HH</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=3, minimum-scale=1">
    
    <meta name="author" content="John Doe">
    
    <meta name="description" content="神经网络显存占用神经网络模型占用的显存主要包含：

模型自身的参数
模型的输出单层DNN网络由Y=XW组成

那么该模型显存占用包括：

参数：二维矩阵W
模型的输出：二维矩阵Y

参数的显存占用只有有参数的层，才会有显存占用。这部份的显存占用和输入无关，模型加载完成之后就会占用
更具体的来说，模型">
    
    
    
    
    <link rel="alternative" href="/Tinnypp/atom.xml" title="The Notes of HH" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/myLogo.png">
    <link rel="apple-touch-icon-precomposed" href="/img/myLogo.png">
    

  
    <link href="/css/font-awesome.min.css" rel="stylesheet">
    
  

    
<link rel="stylesheet" href="/css/style.css">

    <script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?d182ed77fc48758bf45a33835ee35745";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

      <script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
  (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
  e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v1/st.js','_st');

  _st('install','.............Add your swiftype userID...............');
</script>
<meta name="generator" content="Hexo 4.2.0"></head>

  <body>
    <header>
      <div>
		
			<div id="imglogo">
				<a href="/"><img src="/img/logo.svg" alt="The Notes of HH" title="The Notes of HH"/></a>
			</div>
			
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="The Notes of HH">The Notes of HH</a></h1>
				<h2 class="blog-motto">Hold Your Horse</h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="菜单">
			</a></div>
			<nav class="animated">
				<ul>
                    <ul>
					 
						<li><a href="https://hhwode.github.io/">首页</a></li>
					
						<li><a href="https://hhwode.github.io/archives">归档</a></li>
					
					<li>
					
					</li>
                <!--<li><div class="closeaside"><a class="closebutton" href="#" title="隐藏侧边栏"></a></div></li>-->

				</ul>
			</nav>	
</div>
    </header>
    <div id="container" class="clearfix">
      <div id="main" class="post" itemscope itemprop="blogPost">
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2020/02/22/deeplearning/DL在显存中的分析/" title="Deep Learning的模型在GPU显存中的分析" itemprop="url">Deep Learning的模型在GPU显存中的分析</a>
  </h1>
  <p class="article-time">
    <time datetime="2020-02-21T16:00:00.000Z" itemprop="datePublished">2020-02-22</time>
  </p>
</header>
	<div class="article-content">
		
		
		<div id="toc" class="toc-article">
			<strong class="toc-title"></strong>
		<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#神经网络显存占用"><span class="toc-number">1.</span> <span class="toc-text">神经网络显存占用</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#单层DNN网络"><span class="toc-number">1.1.</span> <span class="toc-text">单层DNN网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#参数的显存占用"><span class="toc-number">1.1.1.</span> <span class="toc-text">参数的显存占用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#梯度与动量的显存占用"><span class="toc-number">1.1.2.</span> <span class="toc-text">梯度与动量的显存占用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#输入输出的显存占用"><span class="toc-number">1.1.3.</span> <span class="toc-text">输入输出的显存占用</span></a></li></ol></li></ol></li></ol>
		</div>
		
		<h1 id="神经网络显存占用"><a href="#神经网络显存占用" class="headerlink" title="神经网络显存占用"></a>神经网络显存占用</h1><p>神经网络模型占用的显存主要包含：</p>
<ol>
<li>模型自身的参数</li>
<li>模型的输出<h2 id="单层DNN网络"><a href="#单层DNN网络" class="headerlink" title="单层DNN网络"></a>单层DNN网络</h2>由<code>Y=XW</code>组成</li>
</ol>
<p>那么该模型显存占用包括：</p>
<ol>
<li>参数：二维矩阵<code>W</code></li>
<li>模型的输出：二维矩阵<code>Y</code></li>
</ol>
<h3 id="参数的显存占用"><a href="#参数的显存占用" class="headerlink" title="参数的显存占用"></a>参数的显存占用</h3><p>只有有参数的层，才会有显存占用。这部份的显存占用和<strong>输入无关</strong>，模型加载完成之后就会占用</p>
<p>更具体的来说，模型的参数数目(这里均不考虑偏置项b)为：</p>
<blockquote>
<ol>
<li>Linear(M-&gt;N): 参数数目：M×N</li>
<li>Conv2d(in, out, K): 参数数目：in × out × K × K</li>
<li>BatchNorm(N): 参数数目： 2N</li>
<li>Embedding(N,W): 参数数目： N × W</li>
</ol>
</blockquote>
<p><strong>参数占用显存 = 参数数目×<code>n</code></strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">n &#x3D; 4 ：float32</span><br><span class="line">n &#x3D; 2 : float16</span><br><span class="line">n &#x3D; 8 : double64</span><br></pre></td></tr></table></figure>
<p>在PyTorch中，当你执行完model=MyGreatModel().cuda()之后就会占用相应的显存，占用的显存大小基本与上述分析的显存差不多（会稍大一些，因为其它开销）。</p>
<h3 id="梯度与动量的显存占用"><a href="#梯度与动量的显存占用" class="headerlink" title="梯度与动量的显存占用"></a>梯度与动量的显存占用</h3><p>普通SGD还有梯度值保存，所以显存占用等于参数占用的显存x2<br>如果有动量的SGD，还想保存动量，所以显存占用等于参数占用的显存x3</p>
<p>总结一下，模型中与<strong>输入无关</strong>的显存占用包括：</p>
<blockquote>
<ol>
<li>参数 W</li>
<li>梯度 dW（一般与参数一样）</li>
<li>优化器的动量（普通SGD没有动量，momentum-SGD动量与梯度一样，Adam优化器动量的数量是梯度的两倍）</li>
</ol>
</blockquote>
<h3 id="输入输出的显存占用"><a href="#输入输出的显存占用" class="headerlink" title="输入输出的显存占用"></a>输入输出的显存占用</h3><p>模型输出的显存占用，总结如下：</p>
<blockquote>
<ol>
<li>需要计算每一层的feature map的形状（多维数组的形状）</li>
<li>需要保存输出对应的梯度用以反向传播（链式法则）</li>
<li>显存占用与 batch size 成正比</li>
<li>模型输出不需要存储相应的动量信息。</li>
</ol>
</blockquote>
<p>深度学习中神经网络的显存占用，我们可以得到如下公式：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">显存占用 &#x3D; 模型显存占用 + batch_size × 每个样本的显存占用</span><br></pre></td></tr></table></figure>
<p>可以看出显存不是和batch-size简单的成正比，尤其是模型自身比较复杂的情况下：比如全连接很大，Embedding层很大</p>
  
	</div>
		<footer class="article-footer clearfix">

  <div class="article-tags">
  
  <span></span> <a href="/tags/install/">install</a>
  </div>


<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/deeplearning/">deeplearning</a>►<a class="article-category-link" href="/categories/deeplearning/GPU/">GPU</a>
</div>



<div class="article-share" id="share">

  <div data-url="https://hhwode.github.io/2020/02/22/deeplearning/DL%E5%9C%A8%E6%98%BE%E5%AD%98%E4%B8%AD%E7%9A%84%E5%88%86%E6%9E%90/" data-title="Deep Learning的模型在GPU显存中的分析 | The Notes of HH" data-tsina="" class="share clearfix">
  </div>

</div>
</footer>   	       
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/2020/02/25/hpc/rCUDA/" title="rCUDA Reducing the Number of GPU-Based Accelerators in High Performance Clusters">
  <strong>新一篇:</strong><br/>
  <span>
  rCUDA Reducing the Number of GPU-Based Accelerators in High Performance Clusters</span>
</a>
</div>


<div class="next">
<a href="/2020/02/22/hpc/ZeRO/"  title="ZeRO Memory Optimization Towards Training A Trillion Parameter Models">
 <strong>旧一篇:</strong><br/> 
 <span>ZeRO Memory Optimization Towards Training A Trillion Parameter Models
</span>
</a>
</div>

</nav>

	
<section class="comment">
	
	<div class="ds-thread" data-title="Deep Learning的模型在GPU显存中的分析" data-thread-key="deeplearning/DL在显存中的分析" data-author-key="John Doe" data-url="https://hhwode.github.io/post/deeplearning/DL在显存中的分析"></div>
	
</section>


</div>  
    </div>
    <footer><div id="footer" >
	<div class="copyright">
		<span>Powered by <a href="https://github.com/hexojs/hexo" target="_blank" rel="noopener">Hexo</a> and theme by 
		<a href="https://github.com/levonlin/Tinnypp" target="_blank" rel="noopener">Tinnypp</a>.</span>
		
			<span>© HH</span>
		
	<div>
</div></footer>
    <script src="/js/jquery-2.1.0.min.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  //back to top
  function backToTop(){
    var buttonHTML = $("<a href=\"#top\" id=\"back-top\">" + "<span>Back to Top</span></a>");
    buttonHTML.appendTo($("body"));
    var buttonToTop = $("#back-top");
    // hide #back-top first
    buttonToTop.hide();

    // fade in #back-top
    $(function() {
        $(window).scroll(function() {
            if ($(this).scrollTop() > 200) {
                buttonToTop.fadeIn();
            } else {
                buttonToTop.fadeOut();
            }
        });
        // scroll body to 0px on click
        buttonToTop.click(function() {
            $('body,html').animate({
                scrollTop: 0
            }, 800);
            return false;
        });
    });
  }
  backToTop();

  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      ta = $('#toc.toc-aside');
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else
    {
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
        
    }
  });

  var show = true;
  c.click(function(){
    if(show == true){
        a.addClass('fadeOut').css('display', 'none');
        ta.css('display', 'block').addClass('fadeIn');
        m.addClass('moveMain');  
    }else{
        a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');     
        ta.css('display', 'none'); 
        m.removeClass('moveMain');
        $('#toc.toc-aside').css('display', 'none');
    }
    show = !show;
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      h  = $('article h2')
      ah = $('article h2'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  if(ah.length==0){
    t.css('display','none');
  }else{

    $(window).scroll(function(){
      ta.css("top",Math.max(140,240-$(this).scrollTop()));
    });
  };
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina');
  var html = [
  '<a href="#" class="overlay" id="qrcode"></a>',
  '<div class="qrcode clearfix"><span>扫描二维码分享到微信朋友圈</span><a class="qrclose" href="#share"></a><strong>Loading...Please wait</strong><img id="qrcode-pic" data-src="http://s.jiathis.com/qrcode.php?url=' + encodedUrl + '"/></div>',
  '<a href="#textlogo" class="article-back-to-top" title="Top"></a>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="QRcode"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="Weibo"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);
  $('.article-share-qrcode').click(function(){
    var imgSrc = $('#qrcode-pic').attr('data-src');
    $('#qrcode-pic').attr('src', imgSrc);
    $('#qrcode-pic').load(function(){
        $('.qrcode strong').text(' ');
    });
  });
});     
</script>


<script type="text/javascript">
  var duoshuoQuery = {short_name:"tinnypp"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = '//static.duoshuo.com/embed.js';    //change to ds.src = '/js/embed.js'; to add useragent for duoshuo
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
    || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
</script> 



<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>

<script type="text/javascript">
  function footerPosition() {
    var contentHeight = document.documentElement.scrollHeight,
        winHeight = window.innerHeight;
    if(contentHeight <= winHeight) {
      $('footer').addClass('fixed-bottom');
    } else {
      $('footer').removeClass('fixed-bottom');
    }
  }
  footerPosition();
  $(window).resize(footerPosition);
</script>


  </body>
</html>
