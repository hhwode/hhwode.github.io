
 <!DOCTYPE HTML>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  
    <title>ZeRO Memory Optimization Towards Training A Trillion Parameter Models | The Notes of HH</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=3, minimum-scale=1">
    
    <meta name="author" content="John Doe">
    
    <meta name="description" content="2020-02-24梳理
ZeRO &amp;amp; DeepSpeed1 ZeRO2019年10月微软的几位大佬Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He发表了一篇论文《ZeRO: Memory Optimization To">
    
    
    
    
    <link rel="alternative" href="/Tinnypp/atom.xml" title="The Notes of HH" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/myLogo.png">
    <link rel="apple-touch-icon-precomposed" href="/img/myLogo.png">
    

  
    <link href="/css/font-awesome.min.css" rel="stylesheet">
    
  

    
<link rel="stylesheet" href="/css/style.css">

    <script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?d182ed77fc48758bf45a33835ee35745";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

      <script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
  (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
  e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v1/st.js','_st');

  _st('install','.............Add your swiftype userID...............');
</script>
<meta name="generator" content="Hexo 4.2.0"></head>

  <body>
    <header>
      <div>
		
			<div id="imglogo">
				<a href="/"><img src="/img/logo.svg" alt="The Notes of HH" title="The Notes of HH"/></a>
			</div>
			
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="The Notes of HH">The Notes of HH</a></h1>
				<h2 class="blog-motto">Hold Your Horse</h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="菜单">
			</a></div>
			<nav class="animated">
				<ul>
                    <ul>
					 
						<li><a href="https://hhwode.github.io/">首页</a></li>
					
						<li><a href="https://hhwode.github.io/archives">归档</a></li>
					
					<li>
					
					</li>
                <!--<li><div class="closeaside"><a class="closebutton" href="#" title="隐藏侧边栏"></a></div></li>-->

				</ul>
			</nav>	
</div>
    </header>
    <div id="container" class="clearfix">
      <div id="main" class="post" itemscope itemprop="blogPost">
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2020/02/22/hpc/ZeRO/" title="ZeRO Memory Optimization Towards Training A Trillion Parameter Models" itemprop="url">ZeRO Memory Optimization Towards Training A Trillion Parameter Models</a>
  </h1>
  <p class="article-time">
    <time datetime="2020-02-21T16:00:00.000Z" itemprop="datePublished">2020-02-22</time>
  </p>
</header>
	<div class="article-content">
		
		
		<div id="toc" class="toc-article">
			<strong class="toc-title"></strong>
		<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#ZeRO-amp-DeepSpeed"><span class="toc-number">1.</span> <span class="toc-text">ZeRO &amp; DeepSpeed</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-ZeRO"><span class="toc-number">1.1.</span> <span class="toc-text">1 ZeRO</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1介绍"><span class="toc-number">1.1.1.</span> <span class="toc-text">1.1介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2模型并行"><span class="toc-number">1.1.2.</span> <span class="toc-text">1.2模型并行</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3内存占用分析"><span class="toc-number">1.1.3.</span> <span class="toc-text">1.3内存占用分析</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-1模型状态"><span class="toc-number">1.1.3.1.</span> <span class="toc-text">1.3.1模型状态</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-2Temporary-Buffers"><span class="toc-number">1.1.3.2.</span> <span class="toc-text">1.3.2Temporary Buffers</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4优化技术"><span class="toc-number">1.1.4.</span> <span class="toc-text">1.4优化技术</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-1Pos-Optimizer-State-Partitioning"><span class="toc-number">1.1.4.1.</span> <span class="toc-text">1.4.1Pos : Optimizer State Partitioning</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-2Pg-Gradient-Partitioning"><span class="toc-number">1.1.4.2.</span> <span class="toc-text">1.4.2Pg: Gradient Partitioning</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-3Pp-Parameter-Partitioning"><span class="toc-number">1.1.4.3.</span> <span class="toc-text">1.4.3Pp: Parameter Partitioning</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-5实验"><span class="toc-number">1.1.5.</span> <span class="toc-text">1.5实验</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-5-1吞吐量"><span class="toc-number">1.1.5.1.</span> <span class="toc-text">1.5.1吞吐量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-5-2GPT-2训练"><span class="toc-number">1.1.5.2.</span> <span class="toc-text">1.5.2GPT-2训练</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-5-3总结"><span class="toc-number">1.1.5.3.</span> <span class="toc-text">1.5.3总结</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-DeepSpeed"><span class="toc-number">2.</span> <span class="toc-text">2 DeepSpeed</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-总结"><span class="toc-number">3.</span> <span class="toc-text">3 总结</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Three-Pass"><span class="toc-number">4.</span> <span class="toc-text">Three Pass</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#The-First-Pass"><span class="toc-number">4.1.</span> <span class="toc-text">The First Pass</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Second-Pass"><span class="toc-number">4.2.</span> <span class="toc-text">The Second Pass</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Third-Pass"><span class="toc-number">4.3.</span> <span class="toc-text">The Third Pass</span></a></li></ol></li></ol>
		</div>
		
		<p>2020-02-24梳理</p>
<h1 id="ZeRO-amp-DeepSpeed"><a href="#ZeRO-amp-DeepSpeed" class="headerlink" title="ZeRO &amp; DeepSpeed"></a>ZeRO &amp; DeepSpeed</h1><h2 id="1-ZeRO"><a href="#1-ZeRO" class="headerlink" title="1 ZeRO"></a>1 ZeRO</h2><p>2019年10月微软的几位大佬Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He发表了一篇论文《ZeRO: Memory Optimization Towards Training A Trillion Parameter Models》。主要针对当前模型越大越难训练的困境，主流的模型并行基本都依托单机多卡实现，训练限制比较大；但如果扩展到多机多卡，通信与规模效率都不高，所以作者通过结合数据并行和模型并行的优势来达到对内存优化，并实现了训练超大模型的可能。<br><code>ps：把该文的内存等同于显存，就看使用CPU还是GPU训练。</code></p>
<h3 id="1-1介绍"><a href="#1-1介绍" class="headerlink" title="1.1介绍"></a>1.1介绍</h3><p>深度学习分布式训练的两个基石：应对大数据量的数据并行和应对大参数量的模型并行。两种方式各有利弊：<br>    1. 数据并行：不能解决每个设备内存占用问题，基本上单机训练不了15亿参数的模型；但其容易使用，可大规模部署，计算与通信效率高。<br>    2. 模型并行：由于细粒度计算和昂贵的通信，模型并行性很难有效地扩展到单个节点的多个设备之外，更不用说多个机上；但其可以降低单个设备的内存/显存占用。<br>作者结合数据并行和模型并行的优势来达到内存优化，与跨数据并行进程复制内存状态的基本数据并行性不同，ZeRO对模型状态(也就是训练时模型在显存中的占用部分，包括参数、梯度等)进行分区存储，从而使模型大小与设备数量成线性关系。此外，它通过计算和通信重调度(也就是将一些可重新计算的变量不进行保存，以此降低内存占用)以及降低运行大型模型所需的模型并行度来保持扩展效率。</p>
<h3 id="1-2模型并行"><a href="#1-2模型并行" class="headerlink" title="1.2模型并行"></a>1.2模型并行</h3><p>近年比较常见的模型并行方案基本分两类：pipeline parallelism和Tensor Slicing，其实可以认为是纵向和横向划分网络的方式。<br>    1. pipeline parallelism：需要按层划分，并在数据集上做优化，以此达到一部分数据并行，常见开源框架如GPipe和PipeDream。<br>    2. Tensor Slicing：对Tensor进行切分，是拆分层的一种方式，如Megatron-LM和Mesh-TensorFlow。Nvidia开发的Megatron-LM可以在单个DGX2节点中16张卡上训练200亿参数的模型，并支持多机模型并行，当前只是针对NLP任务。<br>作者不是通过扩展模型并行的规模来训练大模型，而是通过分析训练时内存在保存的是什么，通过对保存的内存进行优化来达到训练大模型的可能性。</p>
<h3 id="1-3内存占用分析"><a href="#1-3内存占用分析" class="headerlink" title="1.3内存占用分析"></a>1.3内存占用分析</h3><p>15亿参数的GPT-2模型，以16位浮点型统计参数，总共3GB，但用pytorch或TensorFlow实现的，却不能在单个32GB的GPU上训练，这是为什么，还有哪些占用了GPU的内存。在模型训练时主要内存消耗在以下几个方面：<br>    1. activations<code>[1]</code>，激活函数部分，待深入了解。<br>    2. OGP states，由优化器状态、参数梯度和参数本身组成的张量。<br>    3. temporary buffers，大模型需要GPU间做gradient all-reduce操作，会有融合缓冲张量产生，也会占一部分内存。<br>这些都是缓存在GPU显存中的，针对activations，如果不缓存，重新计算代价又如何呢，以Megatron-LM实现为例，33%的重新计算开销为代价，可以轻松地消除激活所需的几乎所有内存。基于Megatron-LM已实现，可以重用，作者在本文主要关注后面两个：OGP、temporary buffers。<br>OGP：以Adam优化器为例，主要存储两部分优化器状态<br>    1. 平均动量的时间，<br>    2. 梯度的方差来计算更新，<br>所以用Adam优化器来训练模型，必须保证足够的内存来存储动量和梯度方差的副本，此外还需要内存来存储梯度和权重。</p>
<h4 id="1-3-1模型状态"><a href="#1-3-1模型状态" class="headerlink" title="1.3.1模型状态"></a>1.3.1模型状态</h4><p>模型在内存中的状态主要分三部分：1、优化器状态，如momentum和Adam的方差；2、梯度；3、模型参数。简称模型状态的OGP，O对应优化器，G对应梯度，P对应参数。<br>作者考虑结合数据并行和模型并行的优势，通过数据并行来将这些状态划分到不同设备，以此来降低单个设备的内存占用。即通过跨数据并行进程(而不是复制它们)划分OGP模型状态来消除数据并行进程之间的内存冗余，它通过在训练期间使用动态通信调度来保持数据并行性的计算粒度和通信量，从而保持计算/通信效率。<br>如下图中以Adam优化时，内存消耗分成三部分，参数，梯度，优化器（优化器又分：参数、动量、方差，为什么优化器还细分，需以Adam为例），以混合精度计算：(2+2+K)*，第一部分为参数，以FP16存储，共 2Byte；第二部分为梯度，以FP16存储，共 2Byte，第三部分为优化器，以FP32存储，共Byte。（该种计算方式与之前的有些出入，待分析）<br> <img src="/images/ZeRO/1.png" alt=""><br>如上图，是模型参数总量，K表示优化器状态的内存量，不同优化器，需要量不同，上图中以Adam优化器为例，一个优化器由参数、动量、方差组成。Nd表示数据并行的节点数。当不使用数据并行时，存储的内容还是与正常模型训练一样，只是利用数据并行来降低内存占用。<br> <img src="/images/ZeRO/2.png" alt=""><br>基于其理论分析，如上图，在32GB的V100 GPU上，单卡时，75亿的模型需要占用120GB显存，但利用ZeRO后，在划分模型的OGP状态后，4卡时每个设备只需要保存30GB，1024卡时每个设备只需保存0.12GB，有极大的内存优化收益。理论上提升还是很大，实际呢，下图是实际测试结果。<br> <img src="/images/ZeRO/3.png" alt=""><br>如上图，设备为32GB的V100 GPU，第一列是模型并行使用卡数，第二列是总的卡数，与模型并行卡数相除，可以知道数据并行都是64个节点。中间一列是理论可容纳参数量，右边一列是实际测试容量。<br>Pos表示只划分优化器状态时，以单卡来说，理论只可容纳20亿参数的模型，使用ZeRO的Pos划分后可容纳76亿参数量的模型；实际只能容纳13亿参数，使用ZeRO的Pos划分后可容纳62亿参数。说明内存占用是降下来了，可容纳更大参数量的模型。<br>因为论文只实现了对优化器状态的划分，所以只比了该部分。</p>
<h4 id="1-3-2Temporary-Buffers"><a href="#1-3-2Temporary-Buffers" class="headerlink" title="1.3.2Temporary Buffers"></a>1.3.2Temporary Buffers</h4><p>对于大型模型，用于存储中间结果的临时缓冲区会消耗大量的内存，诸如梯度的all-reduce计算之类的操作，为了提高该操作的处理速度，往往会将所有梯度融合到一个单一的扁平缓冲区中再进行操作，这部分会占一部分内存。</p>
<h3 id="1-4优化技术"><a href="#1-4优化技术" class="headerlink" title="1.4优化技术"></a>1.4优化技术</h3><p>作者明说是结合数据并行与模型并行来达到此优化手段，使用Pytorch框架，Pytorch数据并行方式是Ring All-Reduce，也就是借助ring这个顺序进行划分模型各状态。</p>
<h4 id="1-4-1Pos-Optimizer-State-Partitioning"><a href="#1-4-1Pos-Optimizer-State-Partitioning" class="headerlink" title="1.4.1Pos : Optimizer State Partitioning"></a>1.4.1Pos : Optimizer State Partitioning</h4><p>partitioning：利用数据并行，如果使用Nd节点，那么将Optimizer states等分成Nd份，每个进程只保留并更新1/Nd份Optimizer states，之后在每个训练步骤之后通过all-gather来得到所有进程的参数更新，也就是数据并行越多，保留的越少。</p>
<h4 id="1-4-2Pg-Gradient-Partitioning"><a href="#1-4-2Pg-Gradient-Partitioning" class="headerlink" title="1.4.2Pg: Gradient Partitioning"></a>1.4.2Pg: Gradient Partitioning</h4><p>由于每个数据并行进程只更新其对应的参数分区，因此只需要减少对应参数的梯度，因此，由于每一层的每一个梯度在反向传播时都是可用的，我们只在负责更新相应参数的数据并行过程中减少它们，减少后，我们不再需要梯度和他们的内存可以释放（但计算时需要把）</p>
<h4 id="1-4-3Pp-Parameter-Partitioning"><a href="#1-4-3Pp-Parameter-Partitioning" class="headerlink" title="1.4.3Pp: Parameter Partitioning"></a>1.4.3Pp: Parameter Partitioning</h4><p>正如优化器状态和梯度一样，每个进程只存储与其分区对应的参数。当分区外部的参数需要进行正向和反向传播时，它们通过广播从相关的数据并行进程接收。乍一看，这可能会导致大量的通信开销，但我们证明，这种方法只会将基线数据并行系统的总通信容量增加到1.5倍，同时使内存减少与Nd成比例</p>
<h3 id="1-5实验"><a href="#1-5实验" class="headerlink" title="1.5实验"></a>1.5实验</h3><p>作者只实现了优化器的分区，所以只跟2019年NVIDIA发布的Megatron-LM框架比对，两者比的是规模容量与处理速度。因为ZeRO是以Pytorch实现，所以框架限制比较大。</p>
<h4 id="1-5-1吞吐量"><a href="#1-5-1吞吐量" class="headerlink" title="1.5.1吞吐量"></a>1.5.1吞吐量</h4><p>使用DGX2节点，每节点16个32GB的V100 GPU，比对现有Megatron(NVIDIA开源的大模型训练库，可支持多机多卡分布式)分布式框架，对模型参数大小为8B，20B，40B，60B，80B和100B(一个B就是10亿)的模型进行训练，采用模型并行加数据并行的混合模式。<br> <img src="/images/ZeRO/4.png" alt=""><br>从上图可知，在相同配置下，ZeRO单个GPU处理能力优于Megatron，因为能容纳更多batch size的数据，而且单机模型并行容纳的模型规模更大。</p>
<h4 id="1-5-2GPT-2训练"><a href="#1-5-2GPT-2训练" class="headerlink" title="1.5.2GPT-2训练"></a>1.5.2GPT-2训练</h4><p>15亿参数的GPT-2模型，以16位浮点型(即2Byte)统计参数，使用Adam优化，参数量总共需(2+2+6)*15亿 Byte=15GB内存。单单模型的OGP就需要15GB，单个16GB的GPU不能训练。<br> <img src="/images/ZeRO/5.png" alt=""><br>都是使用相同数量GPU，ZeRO可以不需要进行模型并行就能训练，而Megatron需要进行模型并行才可训练，模型并行本身就极大消耗通信，所以Megatron每秒处理样本数不如ZeRO，大约慢了3.75倍。</p>
<h4 id="1-5-3总结"><a href="#1-5-3总结" class="headerlink" title="1.5.3总结"></a>1.5.3总结</h4><p>论文只是提供了一种训练大模型的思路，大规模模型并行通信效率不高，那就从最基本的降低内存使用上进行优化。而且作者也实现了其中优化器的划分，训练效率上是完胜前段时间NVIDIA开源的Megatron，但论文仅仅是对训练速度上的实验，未对训练收敛、准确性等问题进行验证。<br> <img src="/images/ZeRO/6.png" alt=""><br>此外，如上图，论文实验都是使用高配置，实际操作可能达不到论文结果。</p>
<h1 id="2-DeepSpeed"><a href="#2-DeepSpeed" class="headerlink" title="2 DeepSpeed"></a>2 DeepSpeed</h1><p>DeepSpeed<code>[2]</code>因为开源时间不长，也未进行实际验证，只列出如下几点：<br>    1. DeepSpeed是微软于2020年2月开源的一个深度学习分布式训练库，是2019年10月微软发表的论文《ZeRO: Memory Optimization Towards Training A Trillion Parameter Models》的实现，与pytorch兼容，开源时间不长，文档不全，也不成熟，社区不活跃。<br>    2. DeepSpeed目标是让模型更容易训练，有点类似horovod，需要修改代码适配来跑数据并行和模型并行（代码部分需确定调研才深入），但没介绍什么方式进行数据并行和模型并行。本身DeepSpeed就是再介绍如何结合数据并行和模型并行的优势，来降低内存使用，所以模型并行还是得依赖pytorch的实现。<br>    3. 但其模型并行并未有很大介绍，完全是在介绍如何优化模型，使GPU在训练时占用更少的显存，并结合数据并行来达到训练规模扩大和内存利用率的提高。号称是能单机模型并行训练一万亿参数的模型，如果单机能处理这么大的模型，真不需要进行分布式并行了。<br>    4. DeepSpeed的每个技术都是分开的，单独提供了ZeRO优化技术，可以与其他技术结合使用，如 PipeDream，GPipe，Megatron-LM。</p>
<h1 id="3-总结"><a href="#3-总结" class="headerlink" title="3 总结"></a>3 总结</h1><p>微软该项技术只是结合数据并行和模型并行的一种优化手段，未实现真正意义上的模型并行，如果需要跨机器的多机多卡模型并行，该方案不是可选，但未来如果有对应分布式模型并行，是否可以结合，降低内存消耗呢。<br>ZeRO&amp;DeepSpeed从另外一个角度来看待大模型的训练，直接降低内存占用，通过合理的划分模型在内存中的状态。按作者分析是依据数据并行节点数等分模型各个状态，是否需要层与层之间的联系。此外DeepSpeed由Pytorch实现，框架限制比较大。<br>总之DeepSpeed才刚刚开源，还有许多未知的，有待验证。而且ZeRO论文是与NLP任务的Megatron对比，跑的也是GPT-2模型，不知道是否支持CV领域相关的，未知信息较多。<br>现在模型并行为什么分布式难实现，个人理解：<br>    1. 网络需要划分到不同进程中，真正的分布式实现，但现有方式都是定制化实现，网络划分如何做到自动化呢；Megatron-LM可能是这类，未调研。<br>    2. 如果能将集群GPU统一以一个进程识别，该方式是最理想的，这样无论哪个框架其实都可容易实现分布式模型并行，但如何统一识别，GPU云化？扩大服务器卡槽数？好像有做这方面的<a href="http://www.rcuda.net/index.php/what-s-rcuda.html" target="_blank" rel="noopener">rCUDA</a>，但不是开源的<br>3. 已有的对模型进行压缩，优化，使用半精度保存等等手段，当模型参数巨大时，不是有效解决办法，有其他方式降低模型内存占用吗？GPipe、DeepSpeed都是这类。<br>老铁，还是买更大容量的GPU吧。</p>
<p>Reference<br>    [1]<code>Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. CoRR, abs/1604.06174, 2016.</code><br>    [2]<code>https://github.com/microsoft/DeepSpeed</code></p>
<p>======================================================================================================================================================================================</p>
<p>2019年10月由微软的几位大佬Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He发表的一篇论文。<br>主要针对当前模型越来越大的困境，通过内存优化得技术来达到训练超大模型的可能。<br>（今天日子真是，20200222）</p>
<h1 id="Three-Pass"><a href="#Three-Pass" class="headerlink" title="Three Pass"></a>Three Pass</h1><h2 id="The-First-Pass"><a href="#The-First-Pass" class="headerlink" title="The First Pass"></a>The First Pass</h2><p><strong>列出</strong></p>
<ol>
<li><p>标题，摘要，介绍</p>
<ul>
<li>标题：ZeRO: Memory Optimization Towards Training A Trillion Parameter Models；标题比较直白，通过内存优化技术，来训练1万亿参数的模型，定名是超大模型的训练，现有技术要么特定网络特殊处理，要么使用现有模型并行计算，如mxnet、torch、tf的模型并行，但现有模型并行基本都是单机多卡模型并行，如此大参数量的模型，可能单机存放不下，那如何分布式呢，论文为什么不用分布式模型并行，而仅仅是内存优化就解决了呢。</li>
<li>摘要：训练10亿到万亿参数的模型是很困难的，也是当前DL发展面临的一个困境，现有的解决方案在同时获得内存和规模(计算/通信)效率方面存在限制。<br>1、数据并行：不能解决每个设备内存占用问题，基本上单机训练不了15亿参数的模型<br>2、模型并行：由于细粒度计算和昂贵的通信，模型并行性很难有效地扩展到单个节点的多个设备之外，更不用说多个机上<br>作者另寻僻径，通过优化内存来达到内存和规模的效率提升，<strong>与跨数据并行进程复制内存状态的基本数据并行性不同，它对模型状态进行分区，从而使模型大小与设备数量成线性关系。此外，它通过计算和通信重调度以及降低运行大型模型所需的模型并行度来保持扩展效率。</strong>具体如下，继续更下去。</li>
<li>介绍：相关背景与面临挑战，这其实也是当前DL的一个经验-提高模型的大小能获得更好的准确率，并且介绍NLP中的大模型，Bert-large总共0.3B(3亿参数)，GPT-2总共1.5B(15亿参数)，看来还是NLP模型比较大:-)。<br>同时介绍当前并行训练的两种方式：数据并行和模型并行，其中数据并行虽然能很容易做到规模化，但每个设备加载模型的内存都是一样的，不能帮助我们训练大模型。所以在一个模型不能加载到单个设备上时，就得使用模型并行来进行处理了，现存比较常用的方法有两类：1、pipeline parallelism，如GPipe和PipeDream；2、Tensor Slicing，如Megatron和Mesh TensorFlow，类似Megatron可以在单个DGX2节点中16张卡上训练200亿参数的模型，也是单机多卡模型并行。<br>模型并行面临的困难，即使Megatron可以单机可以处理200亿参数的模型，如果处理1万亿参数量，需要50台机子，总共800张卡来进行模型并行，虽然做到了多机多卡模型并行，但需要付出极大的通信消耗，性能也未必可以提升。<strong>就是即使将模型并行分布式化，也不能有效训练，那是不是可以换一个思路，使单机多卡模型并行就可以训练呢，</strong>这就是作者接下来的想法。</li>
</ul>
<p> <strong>看点一</strong>，分析训练时，内存状态：主要内存占用有三部分，1、优化器状态，如momentum和Adam的方差；2、梯度；3、模型参数。简称模型状态的OGP<br> <strong>看点二</strong>，分析模型并行与数据并行特点：1、数据并行容易规模化，计算/通信效率高，但内存利用率低(此处的利用率是每个设备都是相同模型的副本，使用的都是完全一样的内存占用)；2、模型并行容易获得高内存效率，但通常会导致过于细粒度的计算和昂贵的通信，从而降低了规模化效率。<strong>这两种方法都静态地维护整个训练过程中所需的所有模型状态，即使不是所有的模型状态在训练期间都是必需的</strong><br> 作者提出的方法：Zero Redundacy Optimizer，<strong>通过跨数据并行进程(而不是复制它们)划分OGP模型状态来消除数据并行进程之间的内存冗余，它通过在训练期间使用动态通信调度来保持数据并行性的计算粒度和通信量，从而保持计算/通信效率。</strong>作者把这种方式叫做ZeRO-powered data parallelism。<br> 总结起来就是ZeRO可以减少单个设备的内存占用，以此来达到模型并行的效果，并且作者在单机16卡做模型并行，多机64节点做数据并行，总共1024个GPU训练1万亿参数的模型，即有效率又有规模。<br> <strong>ZeRO三个主要优化阶段</strong>：使用Adam训练，在结合数据并行或混合并行，作者得出结论-对以下三个状态进行分区能节省4,8,Nd倍的内存，1、optimizer states，2、gradients，3、parameters<br> 验证1：1、对Optimizer的优化，没用模型并行，单个32GB的V100 GPU可以处理60亿参数，而单单用pytorch只能处理15亿参数，有4倍提升；2、结合单机模型并行，ZeRO可以处理1000亿参数的模型（到论文这阶段，只实现了对Optimizer的优化，剩下两个还未做）</p>
<p> 这部分介绍挺实在的，不能从一个角度突破，就换另一个思路。</p>
</li>
<li><p>章节</p>
<ul>
<li>1 Extended Introduction</li>
<li>2 Background: Data Parallel Training, Model Parallel Training</li>
<li>3 Where Did All the Memory Go?: Optimizer States, Gradients and Parameters, Temporary Buffers</li>
<li>4 ZeRO: Insights and Overview</li>
<li>5 ZeRO: Memory Optimization: Optimizer State Partitioning, Gradient Partitioning, Parameter Partitioning, Constant Size Buffers, Summary of Memory Optimization</li>
<li>6 ZeRO: Communication Analysis: Data Parallel Communication Volume, ZeRO Communication Volume, Communication Latency</li>
<li>7 ZeRO &amp; Model Parallelism</li>
<li>8 Step Towards 1 Trillion Parameters</li>
<li>9 Implementation and Evaluation of ZeRO-OS: ZeRO-OS Implementation, Evaluation Methodology, Scaling to 6B Parameters with Data-Parallelism Alone, Scaling to 80B-100B Parameters, Up to 6x System Throughput Improvement, Up to 3x Resource Savings</li>
<li>10 Concluding Remarks</li>
<li>References</li>
</ul>
</li>
<li><p>数学原理</p>
<ul>
<li>如何计算模型参数量</li>
<li>时间提升涉及哪些部分</li>
</ul>
</li>
<li><p>结论</p>
<ul>
<li>ZeRO算是一种比较优化得技术，作者还能实现用于实验，也说明他们比较深入该领域，但到工业界使用可能还不成熟，毕竟代码开源不久，而且只实现了Optimizer的一种优化方法，如果继续深入，更成熟点，用在工业界也未尝不可。作者也提到了，该技术还处于初步发展阶段，有待改进与认识，最终目标都是使不可能训练的模型变成可能。</li>
</ul>
</li>
<li><p>文献</p>
<ul>
<li>大型模型，BERT</li>
<li>相关研究，模型并行框架：GPipe、PipeDream、Mesh-TensorFlow（该文献重复写了，文献5、7是一样的，不知道是不是作者漏了一个文献）、Megatron-lm</li>
</ul>
</li>
</ol>
<p><strong>回答</strong></p>
<ol>
<li><p>论文类型</p>
<ul>
<li>系统原型描述与测试，主要是对新提出的一种内存优化技术ZeRO的功能、实现描述，并实验验证理论结果。</li>
</ul>
</li>
<li><p>相关内容</p>
<ul>
<li>模型并行为什么这么难，通信/计算 效率，大规模部署，真正实现应该都具备条件</li>
</ul>
</li>
<li><p>正确性</p>
<ul>
<li>在单机上尽可能训练超大规模的模型，以现阶段，如果真能在单机16卡，每卡32GB的机器上训练1万亿参数的模型，就真不需要跨机器模型并行了</li>
</ul>
</li>
<li><p>贡献点</p>
<ul>
<li>内存优化技术ZeRO，针对Optimizer、Gradient、Parameter进行优化的手段</li>
</ul>
</li>
<li><p>文献情况<br> 已读：<br> [1] Gpipe: Efficient training of giant neural networks using pipeline parallelism. ArXiv, abs/1811.06965, 2018. 还是正常的模型并行手段，只是对mini-batch进行再划分，使训练时能减少GPU间的等待时间，提升一些计算并行度</p>
<p> 未读：<br> [1] Pipedream: Fast and efficient pipeline parallel DNN training. CoRR, abs/1806.03377, 2018.<br> [2] Mesh-tensorflow: Deep learning for supercomputers. CoRR, abs/1811.02084, 2018<br> [3] Megatron-lm: Training multi-billion parameter language models using model parallelism, 2019<br> [4] An empirical model of large-batch training. CoRR, abs/1812.06162, 2018.<br> [5] Training deep nets with sublinear memory cost. CoRR, abs/1604.06174, 2016</p>
</li>
<li><p>是否继续</p>
<ul>
<li>继续，这块是一个很大的坑，如果ZeRO技术成熟应用，单机虽能满足大模型训练，但结合到业务使用，还有很长一段路</li>
</ul>
</li>
</ol>
<h2 id="The-Second-Pass"><a href="#The-Second-Pass" class="headerlink" title="The Second Pass"></a>The Second Pass</h2><p><strong>图表</strong><br>    - 第一章：扩展介绍<br>    - 第二章：背景<br>    DL训练的组成，主要在一个step中处理样本，而一个step包含前向和反向<br>    Data Parallel Training：模型参数是复制到每个GPU的，唯一不同的是mini-batch的数据根据数据并行进程进行拆分，每个进程处理不同的数据，在方向传播是，所有进程的梯度都会求平均，之后再给每个进程处理，也就是OGP状态每个进程都有一份。<br>    Model Parallel Training：主要是减少每个GPU的内存占用，将模型分割为多个部分，并结合多个设备一同执行前向和反向计算，也就是把计算分发到不同设备进行，这个设备进行加，另外一个设备进行减。模型并行主要有两种划分方式-水平划分，垂直划分<br>    - 第三章：分析内存占用在哪块，这部分很主要，也是作者的着力点<br>    分析：15亿参数的GPT-2，如何以16位浮点型统计参数，总共3GB，但用pytorch或TensorFlow实现的，却不能在单个32GB的GPU上训练，这是为什么，还有哪些占用了GPU的内存。在模型训练时主要内存消耗在以下几个方面-1、activations，2、OGP states，由优化器状态、参数梯度和参数本身组成的张量，3、temporary buffers；这些都是缓存在GPU显存中的，针对activations，如果不缓存，重新计算代价又如何呢，以<strong>33%</strong>的重新计算开销为代价，可以轻松地消除激活所需的几乎所有内存，这是Megatron已经实现的，论文作者主要关注后面两个-OGP、temporary buffers<br>    <strong>OGP</strong>：以Adam优化器为例，主要存储两部分优化器状态-1、平均动量的时间，2、梯度的方差来计算更新，所以用Adam优化器来训练模型，必须保证足够的内存来存储<strong>动量</strong>和<strong>梯度方差</strong>的副本，此外还需要内存来存储<strong>梯度</strong>和<strong>权重</strong>，怎么计算的没咋看懂<br>    <strong>Temporary Buffers</strong>：大模型需要GPU间做gradient all-reduce操作，会有融合缓冲张量产生，也会占一部分内存，具体多少，如何计算<br>    - 第四章：ZeRO介绍，看看作者究竟是怎么来减少OGP和temporary buffers的<br>    <strong>如何在不牺牲效率的情况下减少内存占用?</strong>就说明不是简单的将参数放到CPU或提高模型并行的规模，作者主要依据3个关键点：<br>    1、数据并行有比较好的规模效益，数据并行比模型并行在计算效率和通信上更占优势，毕竟是可能多个GPU并行计算<br>    2、数据并行内存效率较低，每个GPU都有所有的模型状态，可能模型状态不是必须的，模型并行就比较好利用内存<br>    3、无论数据并行还是模型并行，所有模型的状态在整个训练过程中都是保存着的，有些只需要在特定时刻使用即可，作者提到每层的参数只在前向和反向有用，这块没搞懂，参数如果不保存在内存中，有其他方式进行重新计算出来吗，但有些状态确实是可以重新计算，不用保存，只是计算和保存哪个收益更高<br>    - 第五章：内存优化，如何优化，如何计算各部分占用情况<br>    <strong>通过对Optimizer states、gradients、parameters进行分区来优化内存</strong><br>    还是利用数据并行来进行减少单个GPU占用，如果我只有一个GPU，可能就不能使用这种技术了，有点类似对数据并行的优化。<br>    【1】Optimizer state partitioning：对于数据并行，如果使用Nd节点，那么将Optimizer states分成Nd份，每个进程只保留并更新1/Nd份Optimizer states，之后再每个训练步骤之后通过all-gather来得到所有进程的参数更新，也就是数据并行越多，保留的越少<br>    【2】Gradient Partitioning：<br>    【3】Parameter Partitioning：</p>
<pre><code>- 第六章：通信问题

- 第七章：模型并行什么时候用-当单独使用数据并行的聚合批处理大小太大而无法获得良好的收敛性时</code></pre><p><strong>相关文献</strong></p>
<h2 id="The-Third-Pass"><a href="#The-Third-Pass" class="headerlink" title="The Third Pass"></a>The Third Pass</h2><p><strong>实验</strong></p>
<p><strong>重现</strong></p>
  
	</div>
		<footer class="article-footer clearfix">

  <div class="article-tags">
  
  <span></span> <a href="/tags/install/">install</a>
  </div>


<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/deeplearning/">deeplearning</a>►<a class="article-category-link" href="/categories/deeplearning/distributedTraining/">distributedTraining</a>
</div>



<div class="article-share" id="share">

  <div data-url="https://hhwode.github.io/2020/02/22/hpc/ZeRO/" data-title="ZeRO Memory Optimization Towards Training A Trillion Parameter Models | The Notes of HH" data-tsina="" class="share clearfix">
  </div>

</div>
</footer>   	       
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/2020/02/22/deeplearning/DL在显存中的分析/" title="Deep Learning的模型在GPU显存中的分析">
  <strong>新一篇:</strong><br/>
  <span>
  Deep Learning的模型在GPU显存中的分析</span>
</a>
</div>


<div class="next">
<a href="/2020/02/21/hpc/DeepSpeed/"  title="DeepSpeed">
 <strong>旧一篇:</strong><br/> 
 <span>DeepSpeed
</span>
</a>
</div>

</nav>

	
<section class="comment">
	
	<div class="ds-thread" data-title="ZeRO Memory Optimization Towards Training A Trillion Parameter Models" data-thread-key="hpc/ZeRO" data-author-key="John Doe" data-url="https://hhwode.github.io/post/hpc/ZeRO"></div>
	
</section>


</div>  
    </div>
    <footer><div id="footer" >
	<div class="copyright">
		<span>Powered by <a href="https://github.com/hexojs/hexo" target="_blank" rel="noopener">Hexo</a> and theme by 
		<a href="https://github.com/levonlin/Tinnypp" target="_blank" rel="noopener">Tinnypp</a>.</span>
		
			<span>© HH</span>
		
	<div>
</div></footer>
    <script src="/js/jquery-2.1.0.min.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  //back to top
  function backToTop(){
    var buttonHTML = $("<a href=\"#top\" id=\"back-top\">" + "<span>Back to Top</span></a>");
    buttonHTML.appendTo($("body"));
    var buttonToTop = $("#back-top");
    // hide #back-top first
    buttonToTop.hide();

    // fade in #back-top
    $(function() {
        $(window).scroll(function() {
            if ($(this).scrollTop() > 200) {
                buttonToTop.fadeIn();
            } else {
                buttonToTop.fadeOut();
            }
        });
        // scroll body to 0px on click
        buttonToTop.click(function() {
            $('body,html').animate({
                scrollTop: 0
            }, 800);
            return false;
        });
    });
  }
  backToTop();

  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      ta = $('#toc.toc-aside');
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else
    {
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
        
    }
  });

  var show = true;
  c.click(function(){
    if(show == true){
        a.addClass('fadeOut').css('display', 'none');
        ta.css('display', 'block').addClass('fadeIn');
        m.addClass('moveMain');  
    }else{
        a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');     
        ta.css('display', 'none'); 
        m.removeClass('moveMain');
        $('#toc.toc-aside').css('display', 'none');
    }
    show = !show;
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      h  = $('article h2')
      ah = $('article h2'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  if(ah.length==0){
    t.css('display','none');
  }else{

    $(window).scroll(function(){
      ta.css("top",Math.max(140,240-$(this).scrollTop()));
    });
  };
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina');
  var html = [
  '<a href="#" class="overlay" id="qrcode"></a>',
  '<div class="qrcode clearfix"><span>扫描二维码分享到微信朋友圈</span><a class="qrclose" href="#share"></a><strong>Loading...Please wait</strong><img id="qrcode-pic" data-src="http://s.jiathis.com/qrcode.php?url=' + encodedUrl + '"/></div>',
  '<a href="#textlogo" class="article-back-to-top" title="Top"></a>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="QRcode"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="Weibo"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);
  $('.article-share-qrcode').click(function(){
    var imgSrc = $('#qrcode-pic').attr('data-src');
    $('#qrcode-pic').attr('src', imgSrc);
    $('#qrcode-pic').load(function(){
        $('.qrcode strong').text(' ');
    });
  });
});     
</script>


<script type="text/javascript">
  var duoshuoQuery = {short_name:"tinnypp"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = '//static.duoshuo.com/embed.js';    //change to ds.src = '/js/embed.js'; to add useragent for duoshuo
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
    || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
</script> 



<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>

<script type="text/javascript">
  function footerPosition() {
    var contentHeight = document.documentElement.scrollHeight,
        winHeight = window.innerHeight;
    if(contentHeight <= winHeight) {
      $('footer').addClass('fixed-bottom');
    } else {
      $('footer').removeClass('fixed-bottom');
    }
  }
  footerPosition();
  $(window).resize(footerPosition);
</script>


  </body>
</html>
